{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-magnet",
   "metadata": {},
   "source": [
    "# Overfit on a Small Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-newton",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "from vidaug import augmentors as vidaug\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from dataset import BasketballDataset\n",
    "from utils.checkpoints import init_session_history, save_weights, load_weights, write_history, read_history, plot_curves\n",
    "from utils.metrics import get_acc_f1_precision_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-liver",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = EasyDict({\n",
    "\n",
    "    'base_model_name': 'r2plus1d_multiclass',\n",
    "\n",
    "    # training/model params\n",
    "    'pretrained': True,\n",
    "    'lr': 0.01,\n",
    "    'start_epoch': 1,\n",
    "    'num_epochs': 25,\n",
    "    'layers_list': ['layer3', 'layer4', 'fc'],\n",
    "    'continue_epoch': False,\n",
    "\n",
    "    # Dataset params\n",
    "    'num_classes': 10,\n",
    "    'batch_size': 8,\n",
    "\n",
    "    # Path params\n",
    "    'annotation_path': \"dataset/annotation_dict.json\",\n",
    "    'augmented_annotation_path': \"dataset/augmented_annotation_dict.json\",\n",
    "    'model_path': \"model/r2plus1d_overfit/\",\n",
    "    'history_path': \"histories/history_r2plus1d_overfit.txt\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-robertson",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, base_model_name, dataloaders, criterion, optimizer, args, start_epoch=1, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Trains the 3D CNN Model\n",
    "    :param model: Model object that we will train\n",
    "    :param base_model_name: The base name of the model\n",
    "    :param dataloaders: A dictionary of train and validation dataloader\n",
    "    :param criterion: Pytorch Criterion Instance\n",
    "    :param optimizer: Pytorch Optimizer Instance\n",
    "    :param num_epochs: Number of epochs during training\n",
    "    :return: model, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_f1_score, val_f1_score, plot_epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Initializes Session History in the history file\n",
    "    init_session_history(args)\n",
    "    since = time.time()\n",
    "\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_f1_score = []\n",
    "    val_f1_score = []\n",
    "    plot_epoch = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                train_pred_classes = []\n",
    "                train_ground_truths = []\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                val_pred_classes = []\n",
    "                val_ground_truths = []\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            train_n_total = 1\n",
    "\n",
    "            pbar = tqdm(dataloaders[phase])\n",
    "            # Iterate over data.\n",
    "            i = 0\n",
    "            for sample in pbar:\n",
    "                inputs = sample[\"video\"]\n",
    "                labels = sample[\"action\"]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    #print(preds)\n",
    "                    #print(torch.max(labels, 1)[1])\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        train_pred_classes.extend(preds.detach().cpu().numpy())\n",
    "                        train_ground_truths.extend(torch.max(labels, 1)[1].detach().cpu().numpy())\n",
    "                    else:\n",
    "                        val_pred_classes.extend(preds.detach().cpu().numpy())\n",
    "                        val_ground_truths.extend(torch.max(labels, 1)[1].detach().cpu().numpy())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == torch.max(labels, 1)[1])\n",
    "\n",
    "                pbar.set_description('Phase: {} || Epoch: {} || Loss {:.5f} '.format(phase, epoch, running_loss / train_n_total))\n",
    "                train_n_total += 1\n",
    "                i+=1\n",
    "                if i==10:\n",
    "                    break\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            # Calculate elapsed time\n",
    "            time_elapsed = time.time() - since\n",
    "            print(phase, ' training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # For Checkpointing and Confusion Matrix\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_pred_classes = np.asarray(val_pred_classes)\n",
    "                val_ground_truths = np.asarray(val_ground_truths)\n",
    "                val_accuracy, val_f1, val_precision, val_recall = get_acc_f1_precision_recall(\n",
    "                    val_pred_classes, val_ground_truths\n",
    "                )\n",
    "                val_f1_score.append(val_f1)\n",
    "                val_confusion_matrix = np.array_str(confusion_matrix(val_ground_truths, val_pred_classes, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "                print('Epoch: {} || Val_Acc: {} || Val_Loss: {}'.format(\n",
    "                    epoch, val_accuracy, epoch_loss\n",
    "                ))\n",
    "                print(f'val: \\n{val_confusion_matrix}')\n",
    "\n",
    "                # Deep Copy Model if best accuracy\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                # set current loss to val loss for write history\n",
    "                val_loss = epoch_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_pred_classes = np.asarray(train_pred_classes)\n",
    "                train_ground_truths = np.asarray(train_ground_truths)\n",
    "                train_accuracy, train_f1, train_precision, train_recall = get_acc_f1_precision_recall(\n",
    "                    train_pred_classes, train_ground_truths\n",
    "                )\n",
    "                train_f1_score.append(train_f1)\n",
    "                train_confusion_matrix = np.array_str(confusion_matrix(train_ground_truths, train_pred_classes, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "                print('Epoch: {} || Train_Acc: {} || Train_Loss: {}'.format(\n",
    "                    epoch, train_accuracy, epoch_loss\n",
    "                ))\n",
    "                print(f'train: \\n{train_confusion_matrix}')\n",
    "                plot_epoch.append(epoch)\n",
    "\n",
    "                # set current loss to train loss for write history\n",
    "                train_loss = epoch_loss\n",
    "\n",
    "        # Save Weights\n",
    "        model_name = save_weights(model, args, epoch, optimizer)\n",
    "\n",
    "        # Write History after train and validation phase\n",
    "        write_history(\n",
    "            args.history_path,\n",
    "            model_name,\n",
    "            train_loss,\n",
    "            val_loss,\n",
    "            train_accuracy,\n",
    "            val_accuracy,\n",
    "            train_f1,\n",
    "            val_f1,\n",
    "            train_precision,\n",
    "            val_precision,\n",
    "            train_recall,\n",
    "            val_recall,\n",
    "            train_confusion_matrix,\n",
    "            val_confusion_matrix\n",
    "        )\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_f1_score, val_f1_score, plot_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/annotation_dict.json\") as f:\n",
    "    annotation_dict = list(json.load(f).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defence - \"block\"\n",
    "stop = [val for val in annotation_dict if val[1] == 0]\n",
    "# Passing\n",
    "ball_pass = [val for val in annotation_dict if val[1] == 1]\n",
    "# Race or Running\n",
    "race = [val for val in annotation_dict if val[1] == 2]\n",
    "# Dribble\n",
    "pallegio = [val for val in annotation_dict if val[1] == 3]\n",
    "# Shooting\n",
    "shooting = [val for val in annotation_dict if val[1] == 4]\n",
    "# Ball In Hand\n",
    "ballinhand = [val for val in annotation_dict if val[1] == 5]\n",
    "# Defensive Position\n",
    "defence = [val for val in annotation_dict if val[1] == 6]\n",
    "# Pick Attempt\n",
    "pick_attempt = [val for val in annotation_dict if val[1] == 7]\n",
    "# No Action - Just standing\n",
    "noaction = [val for val in annotation_dict if val[1] == 8]\n",
    "# walk\n",
    "walk = [val for val in annotation_dict if val[1] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n",
    "print(\"Current Device: \", torch.cuda.current_device())\n",
    "print(\"Device: \", torch.cuda.device(0))\n",
    "print(\"Cuda Is Available: \", torch.cuda.is_available())\n",
    "print(\"Device Count: \", torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "basketball_dataset = BasketballDataset(annotation_dict=args.annotation_path,\n",
    "                                       augmented_dict=args.augmented_annotation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "basketball_dataset[99]['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "basketball_dataset[20]['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 1000):\n",
    "#     if basketball_dataset[i]['class'] == 9:\n",
    "#         print(basketball_dataset[i]['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = len(basketball_dataset)\n",
    "train_dataset_indices = list(range(train_dataset_size))\n",
    "np.random.shuffle(train_dataset_indices)\n",
    "train_idx = train_dataset_indices[:100]\n",
    "test_idx = train_dataset_indices[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = Subset(basketball_dataset, train_idx)\n",
    "test_subset = Subset(basketball_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 50):\n",
    "#     print(train_subset[i][\"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_subset, shuffle=False, batch_size=10)\n",
    "val_loader = DataLoader(dataset=test_subset, shuffle=False, batch_size=10)\n",
    "dataloaders_dict = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize R(2+1)D Model\n",
    "model = models.video.r2plus1d_18(pretrained=args.pretrained, progress=True)\n",
    "\n",
    "# change final fully-connected layer to output 10 classes\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    for layer in args.layers_list:\n",
    "        if layer in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "# input of the next hidden layer\n",
    "num_ftrs = model.fc.in_features\n",
    "# New Model is trained with 128x176 images\n",
    "# Calculation:\n",
    "model.fc = nn.Linear(num_ftrs, args.num_classes, bias=True)\n",
    "print(model)\n",
    "\n",
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-error",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=args.lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.continue_epoch:\n",
    "    model = load_weights(model, args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Put model into device after updating parameters\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "# Train and evaluate\n",
    "model, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_f1_score, val_f1_score, plot_epoch = train_model(model,\n",
    "                                                                                                                                        args.base_model_name,\n",
    "                                                                                                                                        dataloaders_dict,\n",
    "                                                                                                                                        criterion,\n",
    "                                                                                                                                        optimizer_ft,\n",
    "                                                                                                                                        args,\n",
    "                                                                                                                                        start_epoch=args.start_epoch,\n",
    "                                                                                                                                        num_epochs=args.num_epochs)\n",
    "\n",
    "print(\"Best Validation Loss: \", min(val_loss_history), \"Epoch: \", val_loss_history.index(min(val_loss_history)))\n",
    "print(\"Best Training Loss: \", min(train_loss_history), \"Epoch: \", train_loss_history.index(min(train_loss_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Final Curve\n",
    "plot_curves(\n",
    "    args.base_model_name,\n",
    "    train_loss_history,\n",
    "    val_loss_history,\n",
    "    train_acc_history,\n",
    "    val_acc_history,\n",
    "    train_f1_score,\n",
    "    val_f1_score,\n",
    "    plot_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read History\n",
    "read_history(args.history_path)\n",
    "\n",
    "# Check Accuracy with Test Set\n",
    "check_accuracy(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
